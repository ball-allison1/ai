{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2bd56c-f4b7-4ea9-9e6d-dd489f54ab0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STAMPS AI - V2 ; Cross-scope Synthesis (E + I + M)\n",
    "\n",
    "Approach\n",
    "- Retrieves top-k relevant excerpts independently from:\n",
    "  - E-STAMP,\n",
    "  - I-STAMP,\n",
    "  - M-STAMP\n",
    "- All retrieval is performed against authoritative STAMP documents only\n",
    "- No user-provided filters are required\n",
    "\n",
    "- Extracts abstracted themes within each STAMP product\n",
    "- Themes are normalized, non-duplicative, and evidence-backed\n",
    "- Each theme is explicitly linked to supporting document excerpts\n",
    "\n",
    "- Aligns similar themes across E/I/M into canonical cross-STAMP themes \n",
    "- Tracks which products support each theme\n",
    "- Preserves product-specific nuance while removing redundancy\n",
    "\n",
    "- Groups evidence at the theme level\n",
    "- Selects a small number of representative citations per theme\n",
    "- Converts raw excerpts into concise, leadership-friendly evidence bullets\n",
    "- All bullets remain grounded to page-level citations\n",
    "\n",
    "- Identifies coverage gaps where themes appear in some STAMP products but not others\n",
    "- Identifies defintion gaps where themes lack operational clarity\n",
    "- Gaps are explicitly labeled and confidence-scored\n",
    "\n",
    "- Produces a structured narrative including:\n",
    "  - Executive summary\n",
    "  - Cross-STAMP themes\n",
    "  - Identified gaps and inconsistencies\n",
    "  - Implications\n",
    "- All claims are evidence-backed and citation-ready\n",
    "\n",
    "\n",
    "Limitations:\n",
    "- Retrieval uses keyword scoring still\n",
    "- Insights still limited to what is documented in STAMP pdfs\n",
    "- Quantitative questions are still out of scope\n",
    "\n",
    "\n",
    "What's Next:\n",
    "- Output consistency testing across diverse question types\n",
    "- Theme and gap stability validation across repeated runs\n",
    "- Tone and length calibration for leaders\n",
    "- Citation accuracy verification at scale\n",
    "\n",
    "\n",
    "Next Version: \n",
    "- Integrate STAMP data sources alongside documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4afc9e4a-7a5f-434c-b96b-1eafdf6022b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\nCollecting pymupdf\n  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\nCollecting mlflow-skinny==3.8.1 (from mlflow)\n  Downloading mlflow_skinny-3.8.1-py3-none-any.whl.metadata (31 kB)\nCollecting mlflow-tracing==3.8.1 (from mlflow)\n  Downloading mlflow_tracing-3.8.1-py3-none-any.whl.metadata (19 kB)\nCollecting Flask-CORS<7 (from mlflow)\n  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\nCollecting Flask<4 (from mlflow)\n  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting alembic!=1.10.0,<2 (from mlflow)\n  Downloading alembic-1.18.1-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: cryptography<47,>=43.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (43.0.3)\nCollecting docker<8,>=4.0.0 (from mlflow)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting huey<3,>=2.5.0 (from mlflow)\n  Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (3.10.0)\nRequirement already satisfied: numpy<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.1.3)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (2.2.3)\nRequirement already satisfied: pyarrow<23,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.6.1)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.12/site-packages (from mlflow) (1.15.1)\nCollecting sqlalchemy<3,>=1.4.0 (from mlflow)\n  Downloading sqlalchemy-2.0.46-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (5.5.1)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (3.0.0)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.49.0)\nRequirement already satisfied: fastapi<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.115.12)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.43)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (6.6.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.32.1)\nCollecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (1.32.1)\nRequirement already satisfied: packaging<26 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (24.2)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (5.29.4)\nRequirement already satisfied: pydantic<3,>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (2.10.6)\nCollecting python-dotenv<2,>=0.19.0 (from mlflow-skinny==3.8.1->mlflow)\n  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (6.0.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (2.32.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (4.12.2)\nRequirement already satisfied: uvicorn<1 in /databricks/python3/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow) (0.34.2)\nCollecting Mako (from alembic!=1.10.0,<2->mlflow)\n  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.12/site-packages (from cryptography<47,>=43.0.0->mlflow) (1.17.1)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\nCollecting blinker>=1.9.0 (from Flask<4->mlflow)\n  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: jinja2>=3.1.2 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (3.1.5)\nRequirement already satisfied: markupsafe>=2.1.1 in /databricks/python3/lib/python3.12/site-packages (from Flask<4->mlflow) (3.0.2)\nCollecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /databricks/python3/lib/python3.12/site-packages (from graphene<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib<4->mlflow) (3.2.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn<2->mlflow) (3.5.0)\nCollecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow)\n  Downloading greenlet-3.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.21)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (2.40.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /databricks/python3/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow) (0.46.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow) (3.21.0)\nRequirement already satisfied: deprecated>=1.2.6 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (1.2.13)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /databricks/python3/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (0.53b1)\nRequirement already satisfied: annotated-types>=0.6.0 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /databricks/python3/lib/python3.12/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (2.27.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (2025.1.31)\nRequirement already satisfied: h11>=0.8 in /databricks/python3/lib/python3.12/site-packages (from uvicorn<1->mlflow-skinny==3.8.1->mlflow) (0.14.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (1.17.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (5.0.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (4.9.1)\nRequirement already satisfied: anyio<5,>=3.6.2 in /databricks/python3/lib/python3.12/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow) (4.6.2)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.4.8)\nDownloading mlflow-3.8.1-py3-none-any.whl (9.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/9.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.3/9.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.8/9.1 MB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/9.1 MB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.5/9.1 MB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m8.4/9.1 MB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.1/9.1 MB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_skinny-3.8.1-py3-none-any.whl (2.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.5/2.5 MB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m1.8/2.5 MB\u001B[0m \u001B[31m4.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mlflow_tracing-3.8.1-py3-none-any.whl (1.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.5/1.4 MB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/24.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/24.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.5/24.1 MB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:12\u001B[0m\r\u001B[2K   \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/24.1 MB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:07\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.9/24.1 MB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.6/24.1 MB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.2/24.1 MB\u001B[0m \u001B[31m15.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m23.9/24.1 MB\u001B[0m \u001B[31m20.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.1/24.1 MB\u001B[0m \u001B[31m19.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading alembic-1.18.1-py3-none-any.whl (260 kB)\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\nDownloading flask-3.1.2-py3-none-any.whl (103 kB)\nDownloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\nDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\nDownloading huey-2.6.0-py3-none-any.whl (76 kB)\nDownloading sqlalchemy-2.0.46-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/3.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.8/3.3 MB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/3.3 MB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\nDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\nDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading greenlet-3.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (609 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/609.9 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m609.9/609.9 kB\u001B[0m \u001B[31m8.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nDownloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\nDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\nDownloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\nDownloading mako-1.3.10-py3-none-any.whl (78 kB)\nInstalling collected packages: huey, werkzeug, python-dotenv, pymupdf, opentelemetry-proto, Mako, itsdangerous, gunicorn, greenlet, graphql-core, blinker, sqlalchemy, graphql-relay, Flask, docker, graphene, Flask-CORS, alembic, mlflow-tracing, mlflow-skinny, mlflow\n  Attempting uninstall: blinker\n    Found existing installation: blinker 1.7.0\n    Not uninstalling blinker at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9952128f-935c-4514-b87b-ef570b7798a5\n    Can't uninstall 'blinker'. No files were found to uninstall.\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 3.0.1\n    Not uninstalling mlflow-skinny at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-9952128f-935c-4514-b87b-ef570b7798a5\n    Can't uninstall 'mlflow-skinny'. No files were found to uninstall.\nSuccessfully installed Flask-3.1.2 Flask-CORS-6.0.2 Mako-1.3.10 alembic-1.18.1 blinker-1.9.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.1 gunicorn-23.0.0 huey-2.6.0 itsdangerous-2.2.0 mlflow-3.8.1 mlflow-skinny-3.8.1 mlflow-tracing-3.8.1 opentelemetry-proto-1.39.1 pymupdf-1.26.7 python-dotenv-1.2.1 sqlalchemy-2.0.46 werkzeug-3.1.5\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method DBUtils.LibraryHandler.restartPython of Package 'dbutils.library'.>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -U mlflow pymupdf\n",
    "dbutils.library.restartPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149d8ca5-521c-46ff-a704-50c37c2c6640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json, requests\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Set, Tuple\n",
    "from pyspark.sql import functions as F\n",
    "import uuid, re\n",
    "import fitz\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231ec2aa-d96d-4c7c-a0eb-9a8014e5bdf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS stamp_ai_pages;\n",
    "DROP TABLE IF EXISTS stamp_ai_chunks;\n",
    "DROP TABLE IF EXISTS stamp_ai_ingest_log;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe5dd8a-274c-4746-a727-b736d2664564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE stamp_ai_pages (\n",
    "  doc_id STRING,\n",
    "  stamp_family STRING,\n",
    "  stamp_product STRING,\n",
    "  majcom STRING,\n",
    "  installation STRING,\n",
    "  stamp_name STRING,\n",
    "  stamp_version STRING,\n",
    "  source_pdf_path STRING,\n",
    "  page_num INT,\n",
    "  page_text STRING\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS stamp_ai_chunks (\n",
    "  chunk_id STRING,\n",
    "  doc_id STRING,\n",
    "  stamp_family STRING,\n",
    "  stamp_product STRING,\n",
    "  majcom STRING,\n",
    "  installation STRING,\n",
    "  stamp_name STRING,\n",
    "  stamp_version STRING,\n",
    "  source_pdf_path STRING,\n",
    "  page_num INT,\n",
    "  section_title STRING,\n",
    "  figure_refs STRING,\n",
    "  chunk_text STRING\n",
    ");\n",
    "\n",
    "CREATE TABLE stamp_ai_ingest_log (\n",
    "  source_pdf_path STRING,\n",
    "  stamp_version STRING,\n",
    "  ingested_at TIMESTAMP,\n",
    "  doc_id STRING,\n",
    "  status STRING,\n",
    "  error STRING\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d014b460-22ec-42f4-ad9d-b6372ae99272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ROOT = \"s3://usaf-data-tenant-afimsc/FSRM/stamps/raw\"\n",
    "\n",
    "RUN_FILTER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa5aeeb-8c1f-4bbf-9d63-95095b82d332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDFs found: 95\nSameple: ['s3://usaf-data-tenant-afimsc/FSRM/stamps/raw/E/Dorms/DEC2025/E-STAMP - Built Infrastructure - DORMS - DEC2025.pdf', 's3://usaf-data-tenant-afimsc/FSRM/stamps/raw/E/Facilities/DEC2025/E-STAMP - Built Infrastructure - FACILITIES - DEC2025.pdf', 's3://usaf-data-tenant-afimsc/FSRM/stamps/raw/E/TNAP/DEC2025/E-STAMP - Built Infrastructure - TNAP - DEC2025.pdf', 's3://usaf-data-tenant-afimsc/FSRM/stamps/raw/E/Utilities/DEC2025/E-STAMP - Built Infrastructure - UTILITIES - DEC2025.pdf', 's3://usaf-data-tenant-afimsc/FSRM/stamps/raw/I/All/FEB2025/MacDill AFB/I-STAMP - Built Infrastructure - MacDill AFB - FEB2025.pdf']\n"
     ]
    }
   ],
   "source": [
    "def list_pdfs_recursive(root: str):\n",
    "    out = []\n",
    "    stack = [root]\n",
    "    while stack:\n",
    "        cur = stack.pop()\n",
    "        for f in dbutils.fs.ls(cur):\n",
    "            if f.path.lower().endswith(\".pdf\"):\n",
    "                out.append(f.path)\n",
    "            elif f.isDir():\n",
    "                stack.append(f.path)\n",
    "    return sorted(out)\n",
    "\n",
    "pdfs = list_pdfs_recursive(ROOT)\n",
    "pdfs = [str(x) for x in pdfs]\n",
    "print(\"Total PDFs found:\", len(pdfs))\n",
    "print(\"Sameple:\", pdfs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0dbbe63-1205-435d-bbab-e248a4855a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_meta_from_path(path: str):\n",
    "    parts = path.split(\"/\")\n",
    "    raw_i = parts.index(\"raw\")\n",
    "\n",
    "    fam = parts[raw_i + 1]\n",
    "    product = parts[raw_i + 2]\n",
    "    run = parts[raw_i + 3]\n",
    "\n",
    "    org = None\n",
    "    if fam == 'I':\n",
    "        org = parts[raw_i + 4] # installation\n",
    "    elif fam == 'M':\n",
    "        org = parts[raw_i + 4] # majcom\n",
    "\n",
    "    stamp_name = f\"{fam}-STAMP - Built Infrastructure - {product}\"\n",
    "    stamp_version = run\n",
    "\n",
    "    return {\n",
    "        \"stamp_family\": fam,\n",
    "        \"stamp_product\": product,\n",
    "        \"stamp_version\": stamp_version,\n",
    "        \"majcom\": org if fam == 'M' else None,\n",
    "        \"installation\": org if fam == 'I' else None,\n",
    "        \"stamp_name\": stamp_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c7678d-1590-4738-8dcf-6b724f4022df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def already_ingested(pdf_path: str, run: str) -> bool:\n",
    "    cond = (\n",
    "        (F.col(\"source_pdf_path\") == pdf_path) &\n",
    "        (F.col(\"stamp_version\") == run) &\n",
    "        (F.col(\"status\") == 'SUCCESS')\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        spark.table(\"stamp_ai_ingest_log\")\n",
    "        .where(cond)\n",
    "        .limit(1)\n",
    "        .count()\n",
    "        > 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3f6ae7-5128-4463-9697-d3949ec1f44a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid, re\n",
    "import fitz\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "pages_schema = StructType([\n",
    "    StructField(\"doc_id\", StringType(), False),\n",
    "    StructField(\"stamp_family\", StringType(), True),\n",
    "    StructField(\"stamp_product\", StringType(), True),\n",
    "    StructField(\"majcom\", StringType(), True),\n",
    "    StructField(\"installation\", StringType(), True),\n",
    "    StructField(\"stamp_name\", StringType(), True),\n",
    "    StructField(\"stamp_version\", StringType(), True),\n",
    "    StructField(\"source_pdf_path\", StringType(), True),\n",
    "    StructField(\"page_num\", IntegerType(), True),\n",
    "    StructField(\"page_text\", StringType(), True),\n",
    "])\n",
    "\n",
    "chunks_schema = StructType([\n",
    "    StructField(\"chunk_id\", StringType(), False),\n",
    "    StructField(\"doc_id\", StringType(), True),\n",
    "    StructField(\"stamp_family\", StringType(), True),\n",
    "    StructField(\"stamp_product\", StringType(), True),\n",
    "    StructField(\"majcom\", StringType(), True),\n",
    "    StructField(\"installation\", StringType(), True),\n",
    "    StructField(\"stamp_name\", StringType(), True),\n",
    "    StructField(\"stamp_version\", StringType(), True),\n",
    "    StructField(\"source_pdf_path\", StringType(), True),\n",
    "    StructField(\"page_num\", IntegerType(), True),\n",
    "    StructField(\"section_title\", StringType(), True),\n",
    "    StructField(\"figure_refs\", StringType(), True),\n",
    "    StructField(\"chunk_text\", StringType(), True),\n",
    "])\n",
    "\n",
    "def s3_to_local_pdf(s3_path: str) -> str:\n",
    "    tmp_dir = \"dbfs:/tmp/stamps_ai\"\n",
    "    dbutils.fs.mkdirs(tmp_dir)\n",
    "    local_dbfs = f\"{tmp_dir}/{uuid.uuid4().hex}.pdf\"\n",
    "    dbutils.fs.cp(s3_path, local_dbfs, True) # overwrite\n",
    "    return \"/dbfs/\" + local_dbfs.replace(\"dbfs:/\", \"\")\n",
    "\n",
    "def detect_figure_refs(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    refs = re.findall(r\"\\b(?:Figure|Fig\\.)\\s*\\d+(?:[A-Za-z]|\\.\\d+)?\\b\", text)\n",
    "    uniq = sorted(set([r.strip() for r in refs]))\n",
    "    return \", \".join(uniq)\n",
    "\n",
    "def chunk_text(text: str, max_chars: int=1800, overlap: int=250):\n",
    "    text = (text or \"\").strip()\n",
    "    if not text: \n",
    "        return []\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    chunks, cur = [], \"\"\n",
    "    for p in paras:\n",
    "        if len(cur) + len(p) + 2 <= max_chars:\n",
    "            cur = (cur + \"\\n\\n\" + p).strip()\n",
    "        else:\n",
    "            if cur:\n",
    "                chunks.append(cur)\n",
    "            if len(p) > max_chars:\n",
    "                start = 0\n",
    "                while start < len(p):\n",
    "                    end = min(start + max_chars, len(p))\n",
    "                    chunks.append(p[start:end])\n",
    "                    start = max(end - overlap, end)\n",
    "                cur = \"\"\n",
    "            else: \n",
    "                cur = p\n",
    "    \n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "\n",
    "    final = []\n",
    "    for i, c in enumerate(chunks):\n",
    "        if i == 0:\n",
    "            final.append(c)\n",
    "        else:\n",
    "            prev = chunks[i-1]\n",
    "            tail = prev[-overlap:] if len(prev) > overlap else prev\n",
    "            final.append((tail + '\\n\\n' + c).strip())\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88cc7af-4220-49c2-8867-e5eec077d82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_ingest_log(source_pdf_path: str, stamp_version: str, doc_id, status: str, error: str | None):\n",
    "    def esc(s):\n",
    "        return s.replace('\"', '\\\\\"') if s is not None else None\n",
    "    \n",
    "    source_pdf_path = esc(source_pdf_path)\n",
    "    stamp_version = esc(stamp_version)\n",
    "    status = esc(status)\n",
    "    error = esc(error[:2000]) if error else None\n",
    "\n",
    "    doc_id_sql = f'\"{esc(doc_id)}\"' if doc_id else \"NULL\"\n",
    "    error_sql = f'\"{error}\"' if error else \"NULL\"\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "              INSERT INTO stamp_ai_ingest_log\n",
    "              VALUES (\n",
    "                  \"{source_pdf_path}\",\n",
    "                  \"{stamp_version}\",\n",
    "                  current_timestamp(),\n",
    "                  {doc_id_sql},\n",
    "                  \"{status}\",\n",
    "                  {error_sql}\n",
    "              )\n",
    "    \"\"\")\n",
    "\n",
    "def ingest_one_pdf(pdf_path: str):\n",
    "    pdf_pag = str(pdf_path)\n",
    "    meta = parse_meta_from_path(pdf_path)\n",
    "\n",
    "    # optional run filter\n",
    "    if RUN_FILTER and meta['stamp_version'] != RUN_FILTER:\n",
    "        return (\"SKIPPED\", None, f\"RUN_FILTER={RUN_FILTER}\")\n",
    "    \n",
    "    if already_ingested(pdf_path, meta['stamp_version']):\n",
    "        return (\"SKIPPED\", None, 'Already ingested')\n",
    "    \n",
    "    try:\n",
    "        local_pdf = s3_to_local_pdf(pdf_path)\n",
    "        doc = fitz.open(local_pdf)\n",
    "        doc_id = uuid.uuid4().hex\n",
    "\n",
    "        page_rows = []\n",
    "        chunk_rows = []\n",
    "\n",
    "        for i in range(len(doc)):\n",
    "            page_num = i + 1\n",
    "            page_text = doc.load_page(i).get_text(\"text\") or \"\"\n",
    "            page_text = page_text.replace(\"\\u00a0\", \" \").strip()\n",
    "\n",
    "            page_rows.append(Row(\n",
    "                doc_id=doc_id,\n",
    "                stamp_family=str(meta.get('stamp_family') or \"\"),\n",
    "                stamp_product=str(meta.get('stamp_product') or \"\"),\n",
    "                majcom=meta.get(\"majcom\"),\n",
    "                installation=meta.get(\"installation\"),\n",
    "                stamp_name=str(meta.get(\"stamp_name\") or \"\"),\n",
    "                stamp_version=str(meta.get(\"stamp_version\") or \"\"),\n",
    "                source_pdf_path=pdf_path,\n",
    "                page_num=int(page_num),\n",
    "                page_text=page_text\n",
    "            ))\n",
    "\n",
    "            figs = detect_figure_refs(page_text) or \"\"\n",
    "            for c in chunk_text(page_text):\n",
    "                chunk_rows.append(Row(\n",
    "                    chunk_id=uuid.uuid4().hex,\n",
    "                    doc_id=doc_id,\n",
    "                    stamp_family=str(meta.get('stamp_family') or \"\"),\n",
    "                    stamp_product=str(meta.get('stamp_product') or \"\"),\n",
    "                    majcom=meta.get(\"majcom\"),\n",
    "                    installation=meta.get(\"installation\"),\n",
    "                    stamp_name=str(meta.get(\"stamp_name\") or \"\"),\n",
    "                    stamp_version=str(meta.get(\"stamp_version\") or \"\"),\n",
    "                    source_pdf_path=pdf_path,\n",
    "                    page_num=int(page_num),\n",
    "                    section_title=f\"Page {page_num}\",\n",
    "                    figure_refs=figs,\n",
    "                    chunk_text=c\n",
    "                ))\n",
    "\n",
    "        spark.createDataFrame(page_rows, schema=pages_schema).write.mode(\"append\").saveAsTable(\"stamp_ai_pages\")\n",
    "        spark.createDataFrame(chunk_rows, schema=chunks_schema).write.mode(\"append\").saveAsTable(\"stamp_ai_chunks\")\n",
    "\n",
    "        write_ingest_log(\n",
    "            source_pdf_path=pdf_path,\n",
    "            stamp_version=str(meta['stamp_version']),\n",
    "            doc_id=doc_id,\n",
    "            status='SUCCESS',\n",
    "            error=None\n",
    "        )\n",
    "\n",
    "        return (\"SUCCESS\", doc_id, None)\n",
    "        \n",
    "    except Exception as e:\n",
    "        write_ingest_log(\n",
    "            source_pdf_path=pdf_path,\n",
    "            stamp_version=str(meta.get(\"stamp_version\") or \"\"),\n",
    "            doc_id=None,\n",
    "            status=\"FAILED\",\n",
    "            error=str(e)\n",
    "        )\n",
    "\n",
    "        return (\"FAILED\", None, str(e))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06117567-21d7-4d58-81d0-3fb84eeb49e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ingestion. PDF count: 95\n[10/95] success=10 skipped=0 failed=0\n[20/95] success=20 skipped=0 failed=0\n[30/95] success=30 skipped=0 failed=0\n[40/95] success=40 skipped=0 failed=0\n[50/95] success=50 skipped=0 failed=0\n[60/95] success=60 skipped=0 failed=0\n[70/95] success=70 skipped=0 failed=0\n[80/95] success=80 skipped=0 failed=0\n[90/95] success=90 skipped=0 failed=0\n[95/95] success=95 skipped=0 failed=0\nDONE\nsuccess= 95 skipped= 0 failed= 0\n"
     ]
    }
   ],
   "source": [
    "success = skipped = failed = 0\n",
    "pdfs = [str(p) for p in pdfs]\n",
    "\n",
    "print(\"Starting ingestion. PDF count:\", len(pdfs))\n",
    "\n",
    "for idx, p in enumerate(pdfs, start=1):\n",
    "    p = str(p)\n",
    "\n",
    "    try:\n",
    "        status, doc_id, msg = ingest_one_pdf(p)\n",
    "\n",
    "        if status == 'SUCCESS':\n",
    "            success += 1\n",
    "        elif status == 'SKIPPED':\n",
    "            skipped += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(\"\\nFAILED:\", p)\n",
    "            print(\" msg:\", msg)\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        print(\"\\nEXCEPTION:\", p)\n",
    "        print(\" \", str(e))\n",
    "\n",
    "    if idx % 10 == 0 or idx == len(pdfs):\n",
    "        print(f\"[{idx}/{len(pdfs)}] success={success} skipped={skipped} failed={failed}\")\n",
    "\n",
    "print(\"DONE\")\n",
    "print(\"success=\", success, 'skipped=', skipped, 'failed=', failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a35d293-d766-4cdb-aa57-aa6f26899b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>status</th><th>n_files</th></tr></thead><tbody><tr><td>SUCCESS</td><td>95</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "SUCCESS",
         95
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "n_files",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 12
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "n_files",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "status,\n",
    "COUNT(*) AS n_files\n",
    "FROM stamp_ai_ingest_log\n",
    "GROUP BY status\n",
    "ORDER BY n_files DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb8137c-a079-4808-9483-72f71cf598db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ingested_at</th><th>source_pdf_path</th><th>stamp_version</th><th>status</th><th>error</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "ingested_at",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "source_pdf_path",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "stamp_version",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "error",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 13
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ingested_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "source_pdf_path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "stamp_version",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "error",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "ingested_at,\n",
    "source_pdf_path,\n",
    "stamp_version,\n",
    "status,\n",
    "error\n",
    "FROM stamp_ai_ingest_log\n",
    "WHERE status = 'FAILED'\n",
    "ORDER BY ingested_at DESC\n",
    "LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5434e683-e5ed-4f22-93c9-95358035ba9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>stamp_version</th><th>stamp_family</th><th>stamp_product</th><th>n_pdfs</th><th>n_chunks</th></tr></thead><tbody><tr><td>DEC2025</td><td>E</td><td>Dorms</td><td>1</td><td>16</td></tr><tr><td>DEC2025</td><td>E</td><td>Facilities</td><td>1</td><td>19</td></tr><tr><td>DEC2025</td><td>E</td><td>TNAP</td><td>1</td><td>19</td></tr><tr><td>DEC2025</td><td>E</td><td>Utilities</td><td>1</td><td>24</td></tr><tr><td>FEB2025</td><td>I</td><td>All</td><td>2</td><td>34</td></tr><tr><td>SEP2025</td><td>I</td><td>All</td><td>79</td><td>1393</td></tr><tr><td>SEP2025</td><td>M</td><td>All</td><td>10</td><td>200</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DEC2025",
         "E",
         "Dorms",
         1,
         16
        ],
        [
         "DEC2025",
         "E",
         "Facilities",
         1,
         19
        ],
        [
         "DEC2025",
         "E",
         "TNAP",
         1,
         19
        ],
        [
         "DEC2025",
         "E",
         "Utilities",
         1,
         24
        ],
        [
         "FEB2025",
         "I",
         "All",
         2,
         34
        ],
        [
         "SEP2025",
         "I",
         "All",
         79,
         1393
        ],
        [
         "SEP2025",
         "M",
         "All",
         10,
         200
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "stamp_version",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "stamp_family",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "stamp_product",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "n_pdfs",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "n_chunks",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 14
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "stamp_version",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "stamp_family",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "stamp_product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "n_pdfs",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "n_chunks",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "stamp_version,\n",
    "stamp_family,\n",
    "stamp_product,\n",
    "COUNT(DISTINCT source_pdf_path) AS n_pdfs,\n",
    "COUNT(*) AS n_chunks\n",
    "FROM stamp_ai_chunks\n",
    "GROUP BY stamp_version, stamp_family, stamp_product\n",
    "ORDER BY stamp_version, stamp_family, stamp_product;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60890f3-137b-45af-b892-94126496dfed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>stamp_family</th><th>stamp_product</th><th>stamp_version</th><th>avg_chars_per_page</th><th>min_chars_per_page</th><th>max_chars_per_page</th></tr></thead><tbody><tr><td>E</td><td>Dorms</td><td>DEC2025</td><td>2341.5</td><td>944</td><td>3087</td></tr><tr><td>E</td><td>Facilities</td><td>DEC2025</td><td>2504.6666666666665</td><td>919</td><td>3292</td></tr><tr><td>E</td><td>TNAP</td><td>DEC2025</td><td>2728.625</td><td>918</td><td>3916</td></tr><tr><td>E</td><td>Utilities</td><td>DEC2025</td><td>3202.777777777778</td><td>919</td><td>5569</td></tr><tr><td>I</td><td>All</td><td>FEB2025</td><td>1940.35</td><td>1105</td><td>3085</td></tr><tr><td>I</td><td>All</td><td>SEP2025</td><td>1777.5388180764774</td><td>131</td><td>2328</td></tr><tr><td>M</td><td>All</td><td>SEP2025</td><td>1591.5461538461539</td><td>872</td><td>2258</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "E",
         "Dorms",
         "DEC2025",
         2341.5,
         944,
         3087
        ],
        [
         "E",
         "Facilities",
         "DEC2025",
         2504.6666666666665,
         919,
         3292
        ],
        [
         "E",
         "TNAP",
         "DEC2025",
         2728.625,
         918,
         3916
        ],
        [
         "E",
         "Utilities",
         "DEC2025",
         3202.777777777778,
         919,
         5569
        ],
        [
         "I",
         "All",
         "FEB2025",
         1940.35,
         1105,
         3085
        ],
        [
         "I",
         "All",
         "SEP2025",
         1777.5388180764774,
         131,
         2328
        ],
        [
         "M",
         "All",
         "SEP2025",
         1591.5461538461539,
         872,
         2258
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "stamp_family",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "stamp_product",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "stamp_version",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "avg_chars_per_page",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "min_chars_per_page",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "max_chars_per_page",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 15
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "stamp_family",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "stamp_product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "stamp_version",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg_chars_per_page",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "min_chars_per_page",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "max_chars_per_page",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT\n",
    "stamp_family,\n",
    "stamp_product,\n",
    "stamp_version,\n",
    "AVG(LENGTH(page_text)) AS avg_chars_per_page,\n",
    "MIN(LENGTH(page_text)) AS min_chars_per_page,\n",
    "MAX(LENGTH(page_text)) AS max_chars_per_page\n",
    "FROM stamp_ai_pages\n",
    "GROUP BY stamp_family, stamp_product, stamp_version\n",
    "ORDER BY stamp_family, stamp_product, stamp_version;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234a15b5-d3fe-433b-ab9c-f30f8ff6fbf8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "helpers"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# inference helpers\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip().lower())\n",
    "\n",
    "def build_scope_lookup():\n",
    "    \"\"\"\n",
    "    pulling distinct installations and majcom names from indexed chunks so can infer org scope from question \n",
    "    \"\"\"\n",
    "    df = spark.table(\"stamp_ai_chunks\")\n",
    "\n",
    "    installs = (\n",
    "        df.where(F.col(\"stamp_family\") == \"I\")\n",
    "        .select(\"installation\")\n",
    "        .where(F.col(\"installation\").isNotNull() & (F.length(\"installation\") > 0))\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    majcoms = (\n",
    "        df.where(F.col(\"stamp_family\") == \"M\")\n",
    "        .select(\"majcom\")\n",
    "        .where(F.col(\"majcom\").isNotNull() & (F.length(\"majcom\") > 0))\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    installations = sorted({_norm(r['installation']): r['installation'] for r in installs}.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    majcoms = sorted({_norm(r['majcom']): r['majcom'] for r in majcoms}.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "    return installations, majcoms\n",
    "\n",
    "INSTALL_LOOKUP, MAJCOM_LOOKUP = build_scope_lookup()\n",
    "\n",
    "def latest_stamp_version():\n",
    "    \"\"\"\n",
    "    defaulting to latest versions since there's various versions in sharepoint\n",
    "    \"\"\"\n",
    "    r = (\n",
    "        spark.table(\"stamp_ai_chunks\")\n",
    "        .select(\"stamp_version\")\n",
    "        .where(F.col(\"stamp_version\").isNotNull())\n",
    "        .distinct()\n",
    "        .orderBy(F.col(\"stamp_version\").desc())\n",
    "        .limit(1)\n",
    "        .collect()\n",
    "    )\n",
    "    return r[0]['stamp_version'] if r else None\n",
    "\n",
    "PRODUCT_KEYWORDS = {\n",
    "    \"Utilities\": ['utilities', 'electric', 'power', 'water', 'wastewater', 'sewer', 'steam', 'natural gas'],\n",
    "    \"Facilities\": ['facilities', 'facility', 'buildings', 'roof', 'hvac'],\n",
    "    \"Dorms\": ['dorms', 'dorm', 'unaccompanied housing', 'barracks'],\n",
    "    'TNAP': ['tnap', 'tactical', 'airfield', 'pavement', 'runway', 'taxiway', 'apron'],\n",
    "}\n",
    "\n",
    "FAMILY_HINTS = {\n",
    "    \"E\": ['enterprise', 'e-stamp', 'e stamp'],\n",
    "    'I': ['installation', 'i-stamp', 'i stamp', 'base', 'afb', 'afs', 'ab', 'sfs', 'sfb', 'angb'],\n",
    "    'M': ['majcom', 'm-stamp', 'm stamp'],\n",
    "}\n",
    "\n",
    "def infer_product(q: str) -> str | None:\n",
    "    qn = _norm(q)\n",
    "    for prod, kws in PRODUCT_KEYWORDS.items():\n",
    "        if any(k in qn for k in kws):\n",
    "            return prod\n",
    "    return None\n",
    "\n",
    "def infer_family_from_text(q: str) -> str | None:\n",
    "    qn = _norm(q)\n",
    "    for fam, kws in FAMILY_HINTS.items():\n",
    "        if any(k in qn for k in kws):\n",
    "            return fam\n",
    "    return None\n",
    "\n",
    "def infer_org(q: str):\n",
    "    \"\"\"\n",
    "    returns family, ins, majcom if org is detected from lookup tables\n",
    "    installation match forces family = I and majcom match forces family = M\n",
    "    \"\"\"\n",
    "    qn = _norm(q)\n",
    "\n",
    "    for key, canonical in INSTALL_LOOKUP:\n",
    "        if key and key in qn:\n",
    "            return (\"I\", canonical, None)\n",
    "    \n",
    "    for key, canonical in MAJCOM_LOOKUP:\n",
    "        if key and key in qn:\n",
    "            return (\"M\", canonical, None)\n",
    "        \n",
    "    return (None, None, None)\n",
    "\n",
    "def infer_version(q:str) -> str | None:\n",
    "    qn = _norm(q)\n",
    "\n",
    "    versions = (\n",
    "        spark.table(\"stamp_ai_chunks\")\n",
    "        .select(\"stamp_version\")\n",
    "        .where(F.col(\"stamp_version\").isNotNull())\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    known = [v['stamp_version'] for v in versions if v['stamp_version']]\n",
    "    known = sorted(known, key=lambda s: len(s), reverse = True)\n",
    "\n",
    "    for v in known:\n",
    "        if _norm(v) in qn:\n",
    "            return v\n",
    "        return None\n",
    "    \n",
    "# retriever helper\n",
    "\n",
    "def retrieve_top_chunks(\n",
    "    question: str,\n",
    "    stamp_family: str | None=None,\n",
    "    stamp_product: str | None=None,\n",
    "    stamp_version: str | None=None,\n",
    "    installation: str | None=None,\n",
    "    majcom: str | None=None,\n",
    "    top_k: int=8\n",
    "):\n",
    "    base = spark.table(\"stamp_ai_chunks\")\n",
    "\n",
    "    if stamp_family: base = base.filter(F.col(\"stamp_family\") == stamp_family)\n",
    "    if stamp_product: base = base.filter(F.col(\"stamp_product\") == stamp_product)\n",
    "    if stamp_version: base = base.filter(F.col(\"stamp_version\") == stamp_version)\n",
    "    if installation: base = base.filter(F.col('installation') == installation)\n",
    "    if majcom: base = base.filter(F.col('majcom') == majcom)\n",
    "\n",
    "    q = (question or \"\").lower()\n",
    "    terms = [t.strip(\" ,.;:()[]{}!?\\\"'\").lower() for t in q.split() if len(t.strip(\" ,.;:()[]{}!?\\\"'\")) > 3][:18]\n",
    "\n",
    "    score = None\n",
    "    for t in terms:\n",
    "        expr = F.when(F.lower(F.col(\"chunk_text\")).contains(t), 1).otherwise(0)\n",
    "        score = expr if score is None else (score + expr)\n",
    "\n",
    "    ranked = base.withColumn(\"score\", score if score is not None else F.lit(0)) \\\n",
    "        .orderBy(\n",
    "            F.col(\"score\").desc(),\n",
    "            F.col(\"page_num\").asc()\n",
    "        )\n",
    "\n",
    "    rows = ranked.limit(top_k).collect()\n",
    "    return rows\n",
    "\n",
    "@dataclass\n",
    "class EvidenceRef:\n",
    "    product: str\n",
    "    doc_id: str\n",
    "    page: Optional[int]\n",
    "    chunk_id: Optional[str]\n",
    "    snippet: str\n",
    "\n",
    "@dataclass\n",
    "class ThemeItem:\n",
    "    theme: str\n",
    "    description: str\n",
    "    evidence: List[EvidenceRef]\n",
    "\n",
    "def _build_theme_extraction_prompt(product: str, chunk_payloads: List[Dict[str, Any]], max_themes: int=7) -> str:\n",
    "    formatted = []\n",
    "    for idx, c in enumerate(chunk_payloads, start=1):\n",
    "        txt = (c.get(\"text\") or \"\").strip()\n",
    "        txt = re.sub(r\"\\s\", \" \", txt)\n",
    "        snippet = txt[:700]\n",
    "        formatted.append({\n",
    "            \"i\":idx,\n",
    "            \"doc_id\":c.get(\"doc_id\"),\n",
    "            \"page\":c.get(\"page\"),\n",
    "            \"chunk_id\":c.get(\"chunk_id\"),\n",
    "            \"text\":snippet\n",
    "        })\n",
    "\n",
    "    instructions = f\"\"\"\n",
    "    You are extracting THEMES from STAMP {product}-product document exercepts.\n",
    "    \n",
    "    Rules:\n",
    "    - Output MUST be valid JSON (no markdown, no commentary)\n",
    "    - Produce between 4 and {max_themes} themes. Prefer fewer, higher-quality themes.\n",
    "    - Themes must be short noun phrases (3-7 words), normalized (no document-specific phrasing)\n",
    "    - Each theme must have:\n",
    "        - \"theme\": normalized label\n",
    "        - \"description\": 1-2 sentences explaining what the theme means\n",
    "        - \"evidence_idx\": list of excerpt indices that explicityly support the theme\n",
    "    - Do not invent facts. if unclear, omit the theme.\n",
    "    - Evidence must be explicity: only link an excerpt if it directly supports the theme \n",
    "    \n",
    "    Return schema:\n",
    "    {{\n",
    "        \"product\": \"{product}\",\n",
    "        \"themes\": [\n",
    "            {{\n",
    "                \"theme\": \"...\",\n",
    "                \"description\": \"...\",\n",
    "                \"evidence_idx\": [1, 3]\n",
    "            }}\n",
    "            ]\n",
    "    }}\n",
    "    \n",
    "    Excerpts:\n",
    "    {json.dumps(formatted, ensure_ascii=False)}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return instructions\n",
    "\n",
    "def _safe_json_load(s: str) -> Dict[str, Any]:\n",
    "    s = (s or \"\").strip()\n",
    "    match = re.search(r\"\\{.*\\}\", s, flags=re.DOTALL)\n",
    "    if match:\n",
    "        s = match.group(0)\n",
    "    return json.loads(s)\n",
    "\n",
    "def _validate_theme_json(obj: Dict[str, Any], product: str) -> Dict[str, Any]:\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(\"Theme output is not a JSON object\")\n",
    "    if obj.get(\"product\") != product:\n",
    "        obj['product'] = product\n",
    "\n",
    "    themes = obj.get(\"themes\", [])\n",
    "    if not isinstance(themes, list):\n",
    "        raise ValueError(\"Theme output missing non-empty 'themes' list\")\n",
    "\n",
    "    if len(themes) == 0:\n",
    "        obj[\"themes\"] = []\n",
    "        return obj\n",
    "\n",
    "    cleaned = []\n",
    "    for t in themes:\n",
    "        if not isinstance(t, dict):\n",
    "            continue\n",
    "        theme = (t.get(\"theme\") or \"\").strip()\n",
    "        desc = (t.get(\"description\") or \"\").strip()\n",
    "        evidence_idx = t.get(\"evidence_idx\") or []\n",
    "        if not theme or not desc or not isinstance(evidence_idx, list) or len(evidence_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        theme = re.sub(r\"\\s+\", \" \", theme)\n",
    "        theme = theme.strip(\" -\\t\")\n",
    "\n",
    "        cleaned.append({\n",
    "            \"theme\": theme,\n",
    "            \"description\": desc,\n",
    "            \"evidence_idx\": [int(i) for i in evidence_idx if isinstance(i, (int, float, str)) and str(i).isdigit()]\n",
    "        })\n",
    "\n",
    "    if not cleaned:\n",
    "        raise ValueError(\"No valid themes after cleaning and validation\")\n",
    "\n",
    "    obj[\"themes\"] = cleaned\n",
    "    return obj\n",
    "\n",
    "def extract_themes_per_product(\n",
    "    product: str,\n",
    "    chunks: List[Dict[str, Any]],\n",
    "    llm_call_fn,\n",
    "    max_themes: int = 7\n",
    ") -> List[ThemeItem]:\n",
    "    prompt = _build_theme_extraction_prompt(product, chunks, max_themes=max_themes)\n",
    "    raw = llm_call_fn(prompt)\n",
    "\n",
    "    obj = _safe_json_load(raw)\n",
    "    obj = _validate_theme_json(obj, product)\n",
    "\n",
    "    if not obj.get(\"themes\"):\n",
    "        return []\n",
    "\n",
    "    themes_out: List[ThemeItem] = []\n",
    "\n",
    "    for t in obj['themes']:\n",
    "        ev: List[EvidenceRef] = []\n",
    "\n",
    "        for i in t.get(\"evidence_idx\", []):\n",
    "            if isinstance(i, int) and 1 <= i <= len(chunks):\n",
    "                c = chunks[i - 1]\n",
    "                txt = re.sub(r\"\\s+\", \" \", (c.get(\"text\") or \"\").strip())\n",
    "                ev.append(EvidenceRef(\n",
    "                    product=product,\n",
    "                    doc_id=str(c.get(\"doc_id\") or \"\"),\n",
    "                    page=c.get(\"page\"),\n",
    "                    chunk_id=str(c.get(\"chunk_id\") or \"\"),\n",
    "                    snippet=txt[:300]\n",
    "                ))\n",
    "\n",
    "        if ev:\n",
    "            themes_out.append(ThemeItem(\n",
    "                theme=t.get(\"theme\", \"\"),\n",
    "                description=t.get(\"description\", \"\"),\n",
    "                evidence=ev\n",
    "            ))\n",
    "\n",
    "\n",
    "    return themes_out\n",
    "\n",
    "def _rows_to_chunks(rows, product: str) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    for r in rows:\n",
    "        if hasattr(r, \"asDict\"):\n",
    "            r = r.asDict(recursive=True)\n",
    "        if not isinstance(r, dict):\n",
    "            continue \n",
    "\n",
    "        chunks.append({\n",
    "            \"text\": r.get(\"chunk_text\") or r.get(\"text\") or \"\",\n",
    "            \"page\": r.get(\"page\") or r.get(\"page_number\"),\n",
    "            \"doc_id\": r.get(\"doc_id\") or r.get(\"source_doc\") or r.get(\"path\"),\n",
    "            \"chunk_id\": r.get(\"chunk_id\") or r.get(\"id\")\n",
    "        })\n",
    "    return [c for c in chunks if c['text'].strip()]\n",
    "\n",
    "def llm_call_fn(prompt: str, endpoint: str, temperature: float = 0.2, max_tokens: int = 1100) -> str:\n",
    "    ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    host = \"https://\" + ctx.browserHostName().get()\n",
    "    url = f\"{host}/serving-endpoints/{endpoint}/invocations\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {ctx.apiToken().get()}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": float(temperature), \"max_tokens\": int(max_tokens)}\n",
    "    resp = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    data = resp.json()\n",
    "\n",
    "    return data['choices'][0]['message']['content']\n",
    "\n",
    "@dataclass\n",
    "class CanonicalTheme:\n",
    "    canonical_theme: str\n",
    "    canonical_description: str\n",
    "    products: Set[str]\n",
    "    members: List[Dict[str, str]]\n",
    "    evidence: List[EvidenceRef]\n",
    "\n",
    "def align_cross_product_themes(\n",
    "    e_themes: List[ThemeItem],\n",
    "    i_themes: List[ThemeItem],\n",
    "    m_themes: List[ThemeItem],\n",
    "    llm_one_arg_fn\n",
    ") -> List[CanonicalTheme]:\n",
    "    items = []\n",
    "    idx_to_theme: Dict[str, ThemeItem] = {}\n",
    "    for product, themes in [(\"E\", e_themes), (\"I\", i_themes), (\"M\", m_themes)]:\n",
    "        for j, t in enumerate(themes, start=1):\n",
    "            tid = f\"{product}{j}\"\n",
    "            items.append({\n",
    "                \"id\": tid,\n",
    "                \"product\": product,\n",
    "                \"theme\": t.theme,\n",
    "                \"description\": t.description\n",
    "            })\n",
    "            idx_to_theme[tid] = t\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are aligning THEMES across E-, I-, and M-STAMPS.\n",
    "    \n",
    "    Rules:\n",
    "    - Output MUST be valid JSON only (no markdown, no commentary)\n",
    "    - Do not invent new ideas beyond the input themes\n",
    "    - Create canonical themes by grouping semantically similar input themes\n",
    "    - Each input theme id must appear in exactly ONE group\n",
    "    - Canonical theme names: 1-2 sentences, plain English, grounded in grouped themes\n",
    "    - If a theme is unique, it should be a group with one member\n",
    "    \n",
    "    Return schema:\n",
    "    {{\n",
    "        \"groups\": [\n",
    "            {{\n",
    "                \"canonical_theme\": \"...\",\n",
    "                \"canonical_description\": \"...\",\n",
    "                \"members\": ['E1', 'I2', 'M4']\n",
    "            }}]\n",
    "    }}\n",
    "    \n",
    "    Input themes:\n",
    "    {json.dumps(items, ensure_ascii=False)}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    raw = llm_one_arg_fn(prompt)\n",
    "    obj = _safe_json_load(raw)\n",
    "\n",
    "    groups = obj.get(\"groups\")\n",
    "    if not isinstance(groups, list) or not groups:\n",
    "        raise ValueError(\"Step 2 alignment returned no groups\")\n",
    "    out: List[CanonicalTheme] = []\n",
    "\n",
    "    for g in groups:\n",
    "        canon = (g.get(\"canonical_theme\") or \"\").strip()\n",
    "        cdesc = (g.get(\"canonical_description\") or \"\").strip()\n",
    "        members = g.get('members') or []\n",
    "\n",
    "        if not canon or not isinstance(members, list) or len(members) == 0:\n",
    "            continue\n",
    "\n",
    "        products: Set[str] = set()\n",
    "        member_records: List[Dict[str, str]] = []\n",
    "        evidence: List[EvidenceRef] = []\n",
    "\n",
    "        for mid in members:\n",
    "            mid = str(mid).strip()\n",
    "            if mid not in idx_to_theme: \n",
    "                continue\n",
    "            t = idx_to_theme[mid]\n",
    "\n",
    "            prod = mid[0]\n",
    "            products.add(prod)\n",
    "            member_records.append({\"product\": prod, \"theme\": t.theme})\n",
    "            evidence.extend(t.evidence)\n",
    "\n",
    "        seen: Set[Tuple[str, str, Optional[int], Optional[str]]] = set()\n",
    "        deduped: List[EvidenceRef] = []\n",
    "        for ev in evidence:\n",
    "            key = (ev.product, ev.doc_id, ev.page, ev.chunk_id)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            deduped.append(ev)\n",
    "\n",
    "        out.append(CanonicalTheme(\n",
    "            canonical_theme=canon,\n",
    "            canonical_description=cdesc,\n",
    "            products=products,\n",
    "            members=member_records,\n",
    "            evidence=deduped\n",
    "        ))\n",
    "\n",
    "    if not out:\n",
    "        raise ValueError(\"Step 2 had no canonical themes after processing\")\n",
    "    return out\n",
    "\n",
    "@dataclass\n",
    "class EvidenceBullet:\n",
    "    product: str\n",
    "    page: Optional[int]\n",
    "    doc_id: str\n",
    "    chunk_id: Optional[str]\n",
    "    bullet: str\n",
    "\n",
    "@dataclass\n",
    "class ThemeCard:\n",
    "    canonical_theme: str\n",
    "    canonical_description: str\n",
    "    products: List[str]\n",
    "    evidence: List[EvidenceBullet]\n",
    "\n",
    "    @property\n",
    "    def payload(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"canonical_theme\": self.canonical_theme,\n",
    "            \"canonical_description\": self.canonical_description,\n",
    "            \"products\": self.products,\n",
    "            \"evidence\": [\n",
    "                {\n",
    "                    \"product\": eb.product,\n",
    "                    \"page\": eb.page,\n",
    "                    \"doc_id\": eb.doc_id,\n",
    "                    \"chunk_id\": eb.chunk_id,\n",
    "                    \"bullet\": eb.bullet,\n",
    "                }\n",
    "                for eb in self.evidence\n",
    "            ],\n",
    "        }\n",
    "\n",
    "def _dedup_evidence_refs(evidence: List[EvidenceRef]) -> List[EvidenceRef]:\n",
    "    seen: set = set()\n",
    "    out: List[EvidenceRef] = []\n",
    "    for ev in evidence:\n",
    "        key = (ev.product, ev.doc_id, ev.page, ev.chunk_id)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(ev)\n",
    "    return out\n",
    "\n",
    "def _chunk_to_source_id_lookup(all_citations: List[Dict[str, Any]]) -> Dict[Tuple[str, str, Optional[int], Optional[str]], int]:\n",
    "    lookup = {}\n",
    "    for c in all_citations:\n",
    "        fam = c.get(\"stamp_family\")\n",
    "        path = c.get(\"source_pdf_path\") or c.get(\"stamp_name\") or \"\"\n",
    "        page = c.get(\"page_num\")\n",
    "        key = (fam, str(path), page, None)\n",
    "        lookup[key] = c.get(\"source_id\")\n",
    "    return lookup\n",
    "\n",
    "def build_theme_cards(\n",
    "    canonical_themes: List[CanonicalTheme],\n",
    "    llm_one_arg_fn,\n",
    "    max_evidence_per_theme: int=3\n",
    ") -> List[ThemeCard]:\n",
    "    cards: List[ThemeCard] = []\n",
    "\n",
    "    for ct in canonical_themes[:max_evidence_per_theme]:\n",
    "        ev_refs = _dedup_evidence_refs(ct.evidence)\n",
    "        per_prod: Dict[str, List[EvidenceRef]] = {\"E\": [], \"I\": [], \"M\": []}\n",
    "        for ev in ev_refs:\n",
    "            if ev.product in per_prod and len(per_prod[ev.product]) < 2:\n",
    "                per_prod[ev.product].append(ev)\n",
    "\n",
    "        picked: List[EvidenceRef] = []\n",
    "        for prod in ['E', 'I', 'M']:\n",
    "            picked.extend(per_prod[prod])\n",
    "\n",
    "        if len(picked) < max_evidence_per_theme:\n",
    "            for ev in ev_refs:\n",
    "                if ev in picked:\n",
    "                    continue\n",
    "                picked.append(ev)\n",
    "                if len(picked) >= max_evidence_per_theme:\n",
    "                    break\n",
    "        picked = picked[:max_evidence_per_theme]\n",
    "\n",
    "        bullets: List[EvidenceBullet] = []\n",
    "        for ev in picked:\n",
    "            prompt = f\"\"\"\n",
    "            You will write ONE evidence bullet sentence grounded only in the excerpt \n",
    "            \n",
    "            Rules:\n",
    "            - Output plain text only (no quotes, no JSON)\n",
    "            - One sentence, <= 25 words\n",
    "            - Do NOT invent details not in the excerpt\n",
    "            - Keep it leadership-friendly\n",
    "            \n",
    "            Theme: {ct.canonical_theme}\n",
    "            \n",
    "            Excerpt:\n",
    "            {ev.snippet}\n",
    "            \"\"\".strip()\n",
    "\n",
    "            bullet = llm_one_arg_fn(prompt).strip()\n",
    "            bullets.append(EvidenceBullet(\n",
    "                product=ev.product,\n",
    "                page=ev.page,\n",
    "                doc_id=ev.doc_id,\n",
    "                chunk_id=ev.chunk_id,\n",
    "                bullet=bullet\n",
    "            ))\n",
    "        cards.append(ThemeCard(\n",
    "            canonical_theme=ct.canonical_theme,\n",
    "            canonical_description=ct.canonical_description,\n",
    "            products=sorted(list(ct.products)),\n",
    "            evidence=bullets\n",
    "        ))\n",
    "\n",
    "    return cards\n",
    "\n",
    "@dataclass\n",
    "class GapItem:\n",
    "    gap_type: str\n",
    "    theme: str\n",
    "    present_in: List[str]\n",
    "    missing_in: List[str]\n",
    "    impact: str\n",
    "    confidence: str\n",
    "\n",
    "def identify_coverage_gaps(canonical_themes: List[CanonicalTheme],\n",
    "                           llm_one_arg_fn,\n",
    "                           max_themes_to_check: int=10) -> List[GapItem]:\n",
    "    gaps: List[GapItem] = []\n",
    "    all_products = {'E', 'I', 'M'}\n",
    "\n",
    "    for ct in canonical_themes:\n",
    "        present = set(ct.products)\n",
    "        missing = sorted(list(all_products - present))\n",
    "        if missing:\n",
    "            impact = \"Limits cross-product consistency and comparability for enterprise-level decisions\"\n",
    "            confidence = \"high\"\n",
    "            gaps.append(GapItem(\n",
    "                gap_type=\"coverage\",\n",
    "                theme=ct.canonical_theme,\n",
    "                present_in=sorted(list(present)),\n",
    "                missing_in=missing,\n",
    "                impact=impact,\n",
    "                confidence=confidence\n",
    "            ))\n",
    "    return gaps\n",
    "\n",
    "def identify_definition_gaps(\n",
    "    canonical_themes: List[CanonicalTheme],\n",
    "    llm_one_arg_fn,\n",
    "    max_themes_to_check: int = 10\n",
    ") -> List[GapItem]:\n",
    "    \n",
    "    gaps: List[GapItem] = []\n",
    "\n",
    "    ranked = sorted(\n",
    "        canonical_themes,\n",
    "        key = lambda ct: (len(ct.products), len(ct.evidence)),\n",
    "        reverse=True\n",
    "    )[:max_themes_to_check]\n",
    "\n",
    "    for ct in ranked:\n",
    "        ev_refs = _dedup_evidence_refs(ct.evidence)[:3]\n",
    "        combined = \"\\n\\n\".join([f\"[{ev.product} p.{ev.page}] {ev.snippet}\" for ev in ev_refs])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are assessing whether the sources clearly DEFINE the theme in an operational way\n",
    "        \n",
    "        Theme: {ct.canonical_theme}\n",
    "        Theme description: {ct.canonical_description}\n",
    "        \n",
    "        Sources (excerpts):\n",
    "        {combined}\n",
    "        \n",
    "        Task:\n",
    "        Return valid JSON ONLY with these keys:\n",
    "        - \"is_definition_gap\": true/false\n",
    "        - \"why\": one sentence (<= 25 words) grounded in the excerpts\n",
    "        - \"confidence\": \"high\" | \"medium\" | \"low\" \n",
    "        \n",
    "        Definition gap criteria:\n",
    "        - Missing explicity thresholds, criteria, roles/responsibilities, or decision procedure\n",
    "        - Or only general statemetns without operational detail\n",
    "        \n",
    "        IF the excerpts DO provide operational definition, set is_definition_gap=false\n",
    "        \"\"\".strip()\n",
    "\n",
    "        raw = llm_one_arg_fn(prompt)\n",
    "        obj = _safe_json_load(raw)\n",
    "\n",
    "        is_gap = bool(obj.get('is_definition_gap'))\n",
    "        if not is_gap:\n",
    "            continue\n",
    "\n",
    "        why = (obj.get('why') or '').strip\n",
    "        conf = (obj.get('confidence') or 'low').strip().lower()\n",
    "        if conf not in ['high', 'medium', 'low']:\n",
    "            conf = 'low'\n",
    "\n",
    "        gaps.append(GapItem(\n",
    "            gap_type = 'definition',\n",
    "            theme = ct.canonical_theme,\n",
    "            present_in = sorted(list(ct.products)),\n",
    "            missing_in = [],\n",
    "            impact = why if why else \"Theme lacks operational definition\",\n",
    "            confidence = conf\n",
    "        ))\n",
    "\n",
    "    return gaps\n",
    "\n",
    "def build_gap_summary(\n",
    "    canonical_themes: List[CanonicalTheme],\n",
    "    llm_one_arg_fn,\n",
    "    max_definition_checks: int = 10\n",
    ") -> List[GapItem]:\n",
    "    gaps = []\n",
    "    gaps.extend(identify_coverage_gaps(canonical_themes, llm_one_arg_fn))\n",
    "    gaps.extend(identify_coverage_gaps(canonical_themes, llm_one_arg_fn, max_themes_to_check=max_definition_checks))\n",
    "    return gaps\n",
    "\n",
    "def build_exec_synthesis_prompt(question: str, theme_cards: List[ThemeCard], gaps: List[GapItem]) -> str:\n",
    "    tc_payload = []\n",
    "    for tc in theme_cards:\n",
    "        tc_payload.append({\n",
    "            \"theme\": tc.canonical_theme,\n",
    "            \"description\": tc.canonical_description,\n",
    "            \"products\": tc.products,\n",
    "            \"evidence\": [\n",
    "                {\n",
    "                    \"product\": b.product,\n",
    "                    \"page\": b.page,\n",
    "                    \"doc_id\": b.doc_id,\n",
    "                    \"bullet\": b.bullet\n",
    "                } for b in tc.evidence\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    gap_payload = []\n",
    "    for g in gaps:\n",
    "        gap_payload.append({\n",
    "            \"type\": g.gap_type,\n",
    "            \"theme\": g.theme,\n",
    "            \"present_in\": g.present_in,\n",
    "            \"missing_in\": g.missing_in,\n",
    "            \"impact\": g.impact,\n",
    "            \"confidence\": g.confidence\n",
    "        })\n",
    "\n",
    "    return f\"\"\"\n",
    "    Role: You are an executive decision-support analyst synthesizing information across STAMP products (E, I, M) using only the provided sources. Your goal is to produce a reliable, repeatable, executive-ready synthesis suitable for senior leadership review.\n",
    "\n",
    "    Hard rules (non-negotiable):\n",
    "    1. use ONLY the cited sources provided. Do not infer, assume or generalize beyond what is explicitly supported by citations.\n",
    "    2. Evidence attribution rule (global): Only assert that data covers a specific asset type, facilitiy type, mission area, or population if the cited source explicitly names it. If coverage is implied only through adjacent indicators (e.g., bed space, BCI, BUILDER, PRV, condition indices), describe the coverage as implied but not confirmed and list it as a limitation. Do not upgrade implied coverage to confirmed coverage\n",
    "    3. Cross-Product Discipline: if a finding applies to only one STAMP product, state that explicitly. Do not imply enterprise-wide coverage unless supported across products. \n",
    "    4. No recommendations: do not suggest actions, priorities, investments, or next steps. Implications must describe constraints or enablers only.\n",
    "    5. Tone: write in a concise, neutral, executive briefing style. Avoid speculative language (\"appears\", \"likely\", \"suggests\", etc)\n",
    "    6. Conservative Resolution Rule: when evidence across theme cards is inconsistent or ambiguous regarding explicit coverage of an asset or facility type, default to the most conservative interpretation. If any reasonable uncertainty exists about whether coverage is explicity, treat the coverage as not explicitly confirmed and describe it as a limitation.\n",
    "\n",
    "    Required Output Format (STRICT):\n",
    "    Use exactly the following section headers and order. Do not number sections. Do not add or remove sections.\n",
    "    ## Executive summary\n",
    "    - 2-3 short paragraphs\n",
    "    - Summarize what data exists, what it covers, and major limitations\n",
    "    - No bullets\n",
    "    ## Cross-STAMP Themes\n",
    "    - Each theme must have: Bolded theme title, 2-4 sentences synthesizing evidence across products\n",
    "    - No nested bullets\n",
    "    - Do not introduce themes not grounded in citations\n",
    "    ## Gaps and Inconsistences\n",
    "    - Bulleted list only\n",
    "    - Describe missing coverage, inconsistent scope, or unclear definitions\n",
    "    - Explicitly state when coverage is implied but not confirmed\n",
    "    ## Implications\n",
    "    - 2-4 bullets\n",
    "    - Describe what the data enables or constraints\n",
    "    - No recommendations, prioritization, or action language\n",
    "\n",
    "    Additional Constraints:\n",
    "    - Do not claim completeness where evidence is partial\n",
    "    - Do not reconcile contradictions unless explicitly supported by sources\n",
    "    - If evidence is insufficient to answer definitively, state that clearly\n",
    "    - Consistency and accuracy take priority over completeness\n",
    "   \n",
    "    \n",
    "    QUESTION:\n",
    "    {question}\n",
    "    \n",
    "    THEME CARDS (pre-synthesized evidence):\n",
    "    {json.dumps(tc.payload, ensure_ascii=False)}\n",
    "    \n",
    "    GAPS (identified):\n",
    "    {json.dumps(gap_payload, ensure_ascii=False)}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def render_v25_markdown(v25: dict) -> str:\n",
    "    syn = (v25 or {}).get(\"synthesis\", {}) or {}\n",
    "    es = (syn.get(\"executive_summary\", {}) or {}).get(\"text\", \"\") or \"\"\n",
    "    key_points = syn.get(\"key_points\", []) or []\n",
    "    gaps = (v25 or {}).get(\"gaps_and_limits\", []) or []\n",
    "\n",
    "    lines = []\n",
    "    if es.strip():\n",
    "        lines.append(es.strip())\n",
    "        lines.append(\"\")\n",
    "    if key_points:\n",
    "        lines.append(\"## Key Points\")\n",
    "        for kp in key_points:\n",
    "            txt = (kp or {}).get(\"text\", \"\")\n",
    "            if txt and txt.strip():\n",
    "                lines.append(f\"- {txt.strip()}\")\n",
    "        lines.append(\"\")\n",
    "    if gaps:\n",
    "        lines.append(\"## Gaps & Limits\")\n",
    "        for g in gaps:\n",
    "            gtype = (g or {}).get(\"type\", \"\")\n",
    "            desc = (g or {}).get(\"description\", \"\")\n",
    "            if (gtype or desc):\n",
    "                lines.append(f\"- **{gtype}**: {desc}\".strip())\n",
    "        lines.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(lines).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7f715b-ef0d-418c-b9b9-e7d3946e9655",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "helpers pt 2"
    }
   },
   "outputs": [],
   "source": [
    "import json, re, uuid\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def safe_json_loads(s: str):\n",
    "    if not s or not str(s).strip():\n",
    "        raise ValueError(\"Empty LLM output\")\n",
    "\n",
    "    txt = str(s).strip()\n",
    "\n",
    "    m = re.search(r\"```(?:json)?\\s*(.*?)\\s*```\", txt, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        candidate = m.group(1).strip()\n",
    "    else:\n",
    "        candidate = txt\n",
    "\n",
    "    try:\n",
    "        return json.loads(candidate)\n",
    "    except json.JSONDecodeError as e:\n",
    "        start = max(e.pos - 250, 0)\n",
    "        end = min(e.pos + 250, len(candidate))\n",
    "        excerpt = candidate[start:end]\n",
    "        raise ValueError(\n",
    "            f\"LLM return invalid JSON\\n\"\n",
    "            f\"Excerpt around error (pos {e.pos})\\n{excerpt}\\n\\n\"\n",
    "            f\"Raw output (first 1200 chars)\\n{txt[:1200]}\")\n",
    "\n",
    "def rows_to_v25_citations(rows, start_id=1):\n",
    "    citations = []\n",
    "    sid = start_id\n",
    "\n",
    "    for r in rows:\n",
    "        citations.append({\n",
    "            \"citation_id\": f\"S{sid}\",\n",
    "            \"stamp_family\": r.get('stamp_family') if isinstance(r, dict) else getattr(r, 'stamp_family', None),\n",
    "            \"stamp_product\": r.get('stamp_product') if isinstance(r, dict) else getattr(r, 'stamp_product', None),\n",
    "            \"stamp_version\": r.get('stamp_version') if isinstance(r, dict) else getattr(r, 'stamp_version', None),\n",
    "            \"document_id\": r.get(\"doc_id\", \"\") if isinstance(r, dict) else getattr(r, \"doc_id\", \"\"),\n",
    "            \"document_title\": r.get(\"stamp_name\", \"\") if isinstance(r, dict) else getattr(r, \"stamp_name\", \"\"),\n",
    "            \"page\": int(r.get(\"page_num\")) if isinstance(r, dict) and r.get(\"page_num\") is not None else (int(getattr(r, \"page_num\")) if hasattr(r, \"page_num\") and getattr(r, \"page_num\") is not None else None),\n",
    "            \"figure\": r.get(\"figure_refs\") if isinstance(r, dict) else getattr(r, \"figure_refs\", None),\n",
    "            \"chunk_id\": r.get(\"chunk_id\", \"\") if isinstance(r, dict) else getattr(r, \"chunk_id\", \"\"), \n",
    "            \"quote\": \"\"\n",
    "        })\n",
    "        sid += 1\n",
    "\n",
    "    return citations, sid\n",
    "\n",
    "def build_v25_json_prompt(question, theme_cards, gaps, citations, schema_version=\"2.5\"):\n",
    "    run_id = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\") + \"__\" + uuid.uuid4().hex[:6]\n",
    "\n",
    "    cite_view = [\n",
    "        {\n",
    "            \"citation_id\": c[\"citation_id\"],\n",
    "            \"stamp_family\": c[\"stamp_family\"],\n",
    "            \"stamp_product\": c[\"stamp_product\"],\n",
    "            \"stamp_version\": c[\"stamp_version\"],\n",
    "            \"page\": c[\"page\"],\n",
    "            \"chunk_id\": c[\"chunk_id\"],\n",
    "            \"document_title\": c.get(\"document_title\", \"\")\n",
    "        }\n",
    "        for c in citations\n",
    "    ]\n",
    "\n",
    "    json_skeleton = {\n",
    "        \"schema_version\": schema_version,\n",
    "        \"run_id\": run_id,\n",
    "        \"question\": question,\n",
    "        \"retrieval_summary\": {\n",
    "            \"families_considered\": [\"E\", \"I\", \"M\"],\n",
    "            \"chunks_used\": 0,\n",
    "            \"coverage_by_family\": {\"E\": 0, \"I\": 0, \"M\": 0}\n",
    "        },\n",
    "        \"claims\": [],\n",
    "            # {\n",
    "            #     \"claim_id\": \"C1\",\n",
    "            #     \"statement\": \"\",\n",
    "            #     \"support\": [{\"citation_id\":\"S1\",\"relevance\":\"primary\"}],\n",
    "            #     \"confidence\": \"low\",\n",
    "            #     \"notes\": \"\"\n",
    "            # }\n",
    "            # ],\n",
    "            \"synthesis\": {\n",
    "                \"executive_summary\": {\"text\":\"\", \"derived_from_claims\":[]},\n",
    "                \"key_points\": [],\n",
    "                \"recommended_next_questions\": []\n",
    "            },\n",
    "            \"gaps_and_limits\": [],\n",
    "                # {\n",
    "                #     \"gap_id\":\"G1\",\n",
    "                #     \"type\":\"no_evidence\",\n",
    "                #     \"description\": \"\",\n",
    "                #     \"impact\":\"\",\n",
    "                #     \"affected_families\":[\"E\", \"I\", \"M\"],\n",
    "                #     \"related_claims\":[]\n",
    "                # }\n",
    "                # ],\n",
    "                \"citations\": cite_view,\n",
    "                \"evidence_metrics\": {\n",
    "                    \"claims_count\": 0,\n",
    "                    \"citations_count\": len(cite_view),\n",
    "                    \"unique_documents_count\": 0,\n",
    "                    \"coverage_by_family\": {\n",
    "                        \"E\": {\"claims\": 0, \"citations\":0},\n",
    "                        \"I\": {\"claims\": 0, \"citations\":0},\n",
    "                        \"M\": {\"claims\": 0, \"citations\":0}\n",
    "                    },\n",
    "                    \"conflict_detected\": False,\n",
    "                    \"thin_evidence_claim_ids\": [],\n",
    "                    \"overall_confidence\": \"low\"\n",
    "                },\n",
    "                \"validation\": {\n",
    "                    \"schema_version_valid\": True,\n",
    "                    \"all_claims_supported\": True,\n",
    "                    \"no_external_knowledge_used\": True,\n",
    "                    \"all_synthesis_derived_from_claims\": True,\n",
    "                    \"gaps_section_present\": True,\n",
    "                    \"status\":\"valid\",\n",
    "                    \"notes\":\"\"\n",
    "    }\n",
    "    }\n",
    "\n",
    "    return f\"\"\"\n",
    "    You are STAMP AI. \n",
    "    RETURN FORMAT (STRICT):\n",
    "    - Return ONLY one fenced JSON block.\n",
    "    - No text before or after\n",
    "    - Use EXACT key names (match the skeleton exactly)\n",
    "    - Use ONLY double quotes in JSON.\n",
    "    - Do NOT invent citation_id values. Use only the provided citations list.\n",
    "\n",
    "    Output exactly this structure (fill in values; keep keys the same):\n",
    "    {json.dumps(json_skeleton, ensure_ascii=False, indent=2)}\n",
    "\n",
    "\n",
    "    RULES:\n",
    "    1. Every claim MUST have at least 1 support entry with a valid citation_id from citations[]\n",
    "    2. Synthesis text MUST ONLY restate/combine claims, and MUST list derived_from_claims.\n",
    "    3. If not supported: set claims=[] and add a gaps_and_limits entry with type=\"no_evidence\".\n",
    "    4. You may delete the example items in arrays, but keep arrays present (claims can be empty).\n",
    "    5. You MUST keep citations[] as provided (you can include all of them).\n",
    "\n",
    "    INPUTS:\n",
    "    - Theme cards (guidance only): {json.dumps(make_json_safe(theme_cards), ensure_ascii=False)}\n",
    "    - Gap summary (guidance only): {json.dumps(make_json_safe(gaps), ensure_ascii=False)}\n",
    "    \"\"\".strip()\n",
    "\n",
    "from dataclasses import asdict, is_dataclass\n",
    "\n",
    "def make_json_safe(obj):\n",
    "    if is_dataclass(obj):\n",
    "        return asdict(obj)\n",
    "    \n",
    "    if hasattr(obj, \"model_dump\") and callable(getattr(obj, \"model_dump\")):\n",
    "        return obj.model_dump()\n",
    "    if hasattr(obj, \"dict\") and callable(getattr(obj, \"dict\")):\n",
    "        return obj.dict()\n",
    "    if hasattr(obj, \"asDict\") and callable(getattr(obj, \"asDict\")):\n",
    "        return obj.asDict(recursive=True)\n",
    "    if hasattr(obj, \"__dict__\"):\n",
    "        return {k: make_json_safe(v) for k, v in obj.__dict__.items() if not k.startswith(\"_\")}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        return [make_json_safe(x) for x in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): make_json_safe(v) for k, v in obj.items()}\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8efb99-aabc-45a4-99d3-d25ca04f3561",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ask stamp"
    }
   },
   "outputs": [],
   "source": [
    "import json, requests\n",
    "\n",
    "def ask_stamp_xscope(\n",
    "    question: str,\n",
    "    endpoint: str = 'databricks-claude-sonnet-4-5',\n",
    "    top_k_each: int = 6\n",
    "):\n",
    "    q = question or \"\"\n",
    "\n",
    "    try:\n",
    "        version = latest_stamp_version()\n",
    "    except Exception:\n",
    "        version = None\n",
    "    \n",
    "    try:\n",
    "        product = infer_product(q)\n",
    "    except Exception:\n",
    "        product = None\n",
    "\n",
    "    e_rows = retrieve_top_chunks(\n",
    "        question=q,\n",
    "        stamp_family='E',\n",
    "        stamp_product=product,\n",
    "        stamp_version=version,\n",
    "        top_k=top_k_each\n",
    "    )\n",
    "\n",
    "    i_rows = retrieve_top_chunks(\n",
    "        question=q,\n",
    "        stamp_family='I',\n",
    "        stamp_product='All',\n",
    "        stamp_version=version,\n",
    "        top_k=top_k_each\n",
    "    )\n",
    "\n",
    "    m_rows = retrieve_top_chunks(\n",
    "        question=q,\n",
    "        stamp_family='M',\n",
    "        stamp_product='All',\n",
    "        stamp_version=version,\n",
    "        top_k=top_k_each\n",
    "    )\n",
    "\n",
    "    e_chunks = _rows_to_chunks(e_rows, \"E\")\n",
    "    i_chunks = _rows_to_chunks(i_rows, \"I\")\n",
    "    m_chunks = _rows_to_chunks(m_rows, \"M\")\n",
    "\n",
    "    _model = lambda p: llm_call_fn(p, endpoint=endpoint, temperature=0.0, max_tokens=700)\n",
    "\n",
    "    e_themes = extract_themes_per_product(\"E\", e_chunks, _model)\n",
    "    i_themes = extract_themes_per_product(\"I\", i_chunks, _model)\n",
    "    m_themes = extract_themes_per_product(\"M\", m_chunks, _model)\n",
    "\n",
    "    canonical_themes = align_cross_product_themes(e_themes, i_themes, m_themes, _model)\n",
    "    theme_cards = build_theme_cards(canonical_themes, _model, max_evidence_per_theme=3)\n",
    "    gaps = build_gap_summary(canonical_themes, _model, max_definition_checks=8)\n",
    "\n",
    "    # def pack_sources(rows, start_id=1):\n",
    "    #     blocks = []\n",
    "    #     citations = []\n",
    "    #     source_id = start_id\n",
    "    #     for r in rows:\n",
    "    #         blocks.append( f\"\"\"SOURCE {source_id}\n",
    "    #             STAMP: {r['stamp_name']} | Family={r['stamp_family']} Product={r['stamp_product']} Version={r['stamp_version']}\n",
    "    #             ORG: installation={r['installation'] or \"None\"} majcom={r['majcom'] or \"None\"}\n",
    "    #             PAGE: {r['page_num']} | FIGURES: {r['figure_refs'] or \"none\"}\n",
    "    #             TEXT: \n",
    "    #             {r['chunk_text']}\"\"\"\n",
    "    #             )\n",
    "\n",
    "    #         citations.append({\n",
    "    #             \"source_id\": source_id,\n",
    "    #             \"stamp_name\": r['stamp_name'],\n",
    "    #             \"stamp_family\": r['stamp_family'],\n",
    "    #             \"stamp_product\": r['stamp_product'],\n",
    "    #             \"stamp_version\": r['stamp_version'],\n",
    "    #             'installation': r['installation'],\n",
    "    #             'majcom': r['majcom'],\n",
    "    #             \"page_num\": int(r['page_num']) if r['page_num'] is not None else None,\n",
    "    #             \"figure_refs\": r['figure_refs'],\n",
    "    #             \"source_pdf_path\": r['source_pdf_path'],\n",
    "    #         }) \n",
    "    #         source_id += 1\n",
    "        \n",
    "    #     return blocks, citations, source_id\n",
    "    \n",
    "    # e_blocks, e_cites, next_id = pack_sources(e_rows, start_id=1)\n",
    "    # i_blocks, i_cites, next_id = pack_sources(i_rows, start_id=next_id)\n",
    "    # m_blocks, m_cites, next_id = pack_sources(m_rows, start_id=next_id)\n",
    "\n",
    "    # all_blocks = e_blocks + i_blocks + m_blocks\n",
    "    # all_citations = e_cites + i_cites + m_cites\n",
    "\n",
    "    # v2.5 version of pack sources\n",
    "    def _row_to_dict(r):\n",
    "        return r.asDict(recursive=True) if hasattr(r, \"asDict\") else dict(r)\n",
    "    \n",
    "    e_rows_d = [_row_to_dict(r) for r in e_rows]\n",
    "    i_rows_d = [_row_to_dict(r) for r in i_rows]\n",
    "    m_rows_d = [_row_to_dict(r) for r in m_rows]\n",
    "\n",
    "    e_cites, next_id = rows_to_v25_citations(e_rows_d, start_id=1)\n",
    "    i_cites, next_id = rows_to_v25_citations(i_rows_d, start_id=next_id)\n",
    "    m_cites, next_id = rows_to_v25_citations(m_rows_d, start_id=next_id)\n",
    "\n",
    "    all_citations = e_cites + i_cites + m_cites\n",
    "\n",
    "    if len(all_citations) == 0: \n",
    "        run_id = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M%SZ\") + \"__\" + uuid.uuid4().hex[:6]\n",
    "        v25 = {\n",
    "            \"schema_version\": \"2.5\",\n",
    "            \"run_id\": run_id,\n",
    "            \"question\": q,\n",
    "            \"retrieval_summary\": {\n",
    "                \"families_considered\": ['E', 'I', 'M'],\n",
    "                \"chunks_used\": 0,\n",
    "                \"coverage_by_family\": {'E':0, 'I':0, 'M':0}\n",
    "            },\n",
    "            \"claims\": [],\n",
    "            \"synthesis\": {\n",
    "                \"executive_summary\": {'text': \"Not supported by the STAMP for this query.\", \"derived_from_claims\": []},\n",
    "                \"key_points\": [],\n",
    "                \"recommended_next_questions\": []\n",
    "            },\n",
    "            \"gaps_and_limits\": [{\n",
    "                \"gap_id\": \"G1\",\n",
    "                \"type\": \"no_evidence\",\n",
    "                \"description\": \"No relevant STAMP chunks were retrieved for the question.\",\n",
    "                \"impact\": \"The system cannot answer without STAMP support.\",\n",
    "                \"affected_families\": ['E', 'I', 'M'],\n",
    "                \"related_claims\": []\n",
    "            }],\n",
    "            \"citations\": [],\n",
    "            \"evidence_metrics\": {\n",
    "                \"claims_count\": 0,\n",
    "                \"citations_count\": 0,\n",
    "                \"unique_documents_count\": 0,\n",
    "                \"coverage_by_family\": {\"E\":{\"claims\":0, \"citations\":0}, \"I\":{\"claims\":0, \"citations\":0}, \"M\":{\"claims\":0, \"citations\":0}},\n",
    "                \"conflict_detected\": False,\n",
    "                \"thin_evidence_claim_ids\": [],\n",
    "                \"overall_confidence\": \"low\"\n",
    "            },\n",
    "            \"validation\": {\n",
    "                \"schema_version_valid\": True,\n",
    "                \"all_claims_supported\": True,\n",
    "                \"no_external_knowledge_used\": True,\n",
    "                \"all_synthesis_derived_from_claims\": True,\n",
    "                \"gaps_section_present\": True,\n",
    "                \"status\": \"valid\",\n",
    "                \"notes\": ''\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"v25\": v25, \n",
    "            \"markdown\": render_v25_markdown(v25),\n",
    "            \"debug\": {\"version\": version, \"product\": product, \"counts\": {\"E\": len(e_rows), \"I\": len(i_rows), \"M\": len(m_rows)}}}\n",
    "\n",
    "    # if len(all_blocks) == 0:\n",
    "    #     return {\n",
    "    #         \"answer\": \"Not supported by the STAMPS\",\n",
    "    #         \"citations\": [],\n",
    "    #         \"debug\": {\"version\": version, \"product\": product}\n",
    "    #     }\n",
    "\n",
    "    # prompt = build_exec_synthesis_prompt(question, theme_cards, gaps)\n",
    "    v25_prompt = build_v25_json_prompt(q, theme_cards, gaps, all_citations, schema_version=\"2.5\")\n",
    "    #answer = llm_call_fn(prompt, endpoint=endpoint, temperature=0.2, max_tokens=1100)\n",
    "    raw = llm_call_fn(v25_prompt, endpoint=endpoint, temperature=0.0, max_tokens=1800)\n",
    "\n",
    "    v25 = safe_json_loads(raw)\n",
    "\n",
    "    # return {\n",
    "    #     \"answer\": answer,\n",
    "    #     \"citations\": all_citations,\n",
    "    #     \"debug\": {\"version\": version, \"product\": product, \"counts\": {\"E\": len(e_rows), \"I\": len(i_rows), \"M\": len(m_rows)}}\n",
    "    # }\n",
    "\n",
    "    return {\n",
    "        \"v25\": v25,\n",
    "        \"markdown\": render_v25_markdown(v25),\n",
    "        \"debug\": {\"version\": version, \"product\": product, \"counts\": {\"E\": len(e_rows), \"I\": len(i_rows), \"M\": len(m_rows)}}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc8245f-a1b8-49b7-aafe-5b003611a9b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "clean"
    }
   },
   "outputs": [],
   "source": [
    "# def print_xscope(out):\n",
    "#     print(out['answer'])\n",
    "#     print('\\nCitations (by bucket):')\n",
    "#     for c in out.get('citations', []):\n",
    "#         org = c.get('installation') or c.get('majcom') or ''\n",
    "#         org = f\" | {org}\" if org else \"\"\n",
    "#         fig = f\", {c['figure_refs']}\" if c.get(\"figure_refs\") else ''\n",
    "#         print(f\"- {c['stamp_family']}/{c['stamp_product']} {c['stamp_version']}{org} (p.{c['page_num']}{fig})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc961f7a-d3ce-49c1-b122-90dd9b653635",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "output"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5522249380927693>, line 17\u001B[0m, in \u001B[0;36msafe_json_loads\u001B[0;34m(s)\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m json\u001B[38;5;241m.\u001B[39mloads(candidate)\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m json\u001B[38;5;241m.\u001B[39mJSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n",
       "\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n",
       "\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n",
       "\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_decoder\u001B[38;5;241m.\u001B[39mdecode(s)\n",
       "\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n",
       "\u001B[1;32m    333\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n",
       "\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03mcontaining a JSON document).\u001B[39;00m\n",
       "\u001B[1;32m    335\u001B[0m \n",
       "\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 337\u001B[0m obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw_decode(s, idx\u001B[38;5;241m=\u001B[39m_w(s, \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mend())\n",
       "\u001B[1;32m    338\u001B[0m end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/json/decoder.py:355\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n",
       "\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj, end\n",
       "\n",
       "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1479720345622387>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# out = ask_stamp_xscope(\"what utilities risks are consistent across the enterprise, installations, and MAJCOMs and where do they differ?\")\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m out \u001B[38;5;241m=\u001B[39m ask_stamp_xscope(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat utilitites condition data exists across STAMP products?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# out = ask_stamp_xscope(\"Across scopes, what are the top recurring drivers of infrastructure risk?\")\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m display(out[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmarkdown\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\n",
       "File \u001B[0;32m<command-1479720345622385>, line 171\u001B[0m, in \u001B[0;36mask_stamp_xscope\u001B[0;34m(question, endpoint, top_k_each)\u001B[0m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;66;03m#answer = llm_call_fn(prompt, endpoint=endpoint, temperature=0.2, max_tokens=1100)\u001B[39;00m\n",
       "\u001B[1;32m    169\u001B[0m raw \u001B[38;5;241m=\u001B[39m llm_call_fn(v25_prompt, endpoint\u001B[38;5;241m=\u001B[39mendpoint, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, max_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1800\u001B[39m)\n",
       "\u001B[0;32m--> 171\u001B[0m v25 \u001B[38;5;241m=\u001B[39m safe_json_loads(raw)\n",
       "\u001B[1;32m    173\u001B[0m \u001B[38;5;66;03m# return {\u001B[39;00m\n",
       "\u001B[1;32m    174\u001B[0m \u001B[38;5;66;03m#     \"answer\": answer,\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m \u001B[38;5;66;03m#     \"citations\": all_citations,\u001B[39;00m\n",
       "\u001B[1;32m    176\u001B[0m \u001B[38;5;66;03m#     \"debug\": {\"version\": version, \"product\": product, \"counts\": {\"E\": len(e_rows), \"I\": len(i_rows), \"M\": len(m_rows)}}\u001B[39;00m\n",
       "\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# }\u001B[39;00m\n",
       "\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n",
       "\u001B[1;32m    180\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv25\u001B[39m\u001B[38;5;124m\"\u001B[39m: v25,\n",
       "\u001B[1;32m    181\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmarkdown\u001B[39m\u001B[38;5;124m\"\u001B[39m: render_v25_markdown(v25),\n",
       "\u001B[1;32m    182\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdebug\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion\u001B[39m\u001B[38;5;124m\"\u001B[39m: version, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct\u001B[39m\u001B[38;5;124m\"\u001B[39m: product, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcounts\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlen\u001B[39m(e_rows), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlen\u001B[39m(i_rows), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlen\u001B[39m(m_rows)}}\n",
       "\u001B[1;32m    183\u001B[0m }\n",
       "\n",
       "File \u001B[0;32m<command-5522249380927693>, line 22\u001B[0m, in \u001B[0;36msafe_json_loads\u001B[0;34m(s)\u001B[0m\n",
       "\u001B[1;32m     20\u001B[0m end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(e\u001B[38;5;241m.\u001B[39mpos \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m250\u001B[39m, \u001B[38;5;28mlen\u001B[39m(candidate))\n",
       "\u001B[1;32m     21\u001B[0m excerpt \u001B[38;5;241m=\u001B[39m candidate[start:end]\n",
       "\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m     23\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLLM return invalid JSON\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     24\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExcerpt around error (pos \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;241m.\u001B[39mpos\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mexcerpt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     25\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaw output (first 1200 chars)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mtxt[:\u001B[38;5;241m1200\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: LLM return invalid JSON\n",
       "Excerpt around error (pos 0)\n",
       "```json\n",
       "{\n",
       "  \"schema_version\": \"2.5\",\n",
       "  \"run_id\": \"2026-01-28T20:43:04Z__bb5893\",\n",
       "  \"question\": \"What utilitites condition data exists across STAMP products?\",\n",
       "  \"retrieval_summary\": {\n",
       "    \"families_considered\": [\n",
       "      \"E\",\n",
       "      \"I\",\n",
       "      \"M\"\n",
       "    ]\n",
       "\n",
       "Raw output (first 1200 chars)\n",
       "```json\n",
       "{\n",
       "  \"schema_version\": \"2.5\",\n",
       "  \"run_id\": \"2026-01-28T20:43:04Z__bb5893\",\n",
       "  \"question\": \"What utilitites condition data exists across STAMP products?\",\n",
       "  \"retrieval_summary\": {\n",
       "    \"families_considered\": [\n",
       "      \"E\",\n",
       "      \"I\",\n",
       "      \"M\"\n",
       "    ],\n",
       "    \"chunks_used\": 12,\n",
       "    \"coverage_by_family\": {\n",
       "      \"E\": 0,\n",
       "      \"I\": 6,\n",
       "      \"M\": 6\n",
       "    }\n",
       "  },\n",
       "  \"claims\": [\n",
       "    {\n",
       "      \"claim_id\": \"C1\",\n",
       "      \"claim_text\": \"I-STAMP does not contain specific utilities condition data; it focuses on transportation and airfield infrastructure sourced from PAVER, BUILDER, RAILER, FHWA Bridge Report, and RPA Export through August 2025.\",\n",
       "      \"claim_type\": \"factual\",\n",
       "      \"stamp_family\": \"I\",\n",
       "      \"stamp_products\": [\n",
       "        \"All\"\n",
       "      ],\n",
       "      \"confidence\": \"high\",\n",
       "      \"support\": [\n",
       "        {\n",
       "          \"citation_id\": \"S1\",\n",
       "          \"excerpt\": \"Transportation and airfield infrastructure data sourced from PAVER, BUILDER, RAILER, FHWA Bridge Report, and RPA Export through August 2025.\",\n",
       "          \"relevance\": \"direct\"\n",
       "        },\n",
       "        {\n",
       "          \"citation_id\": \"S2\",\n",
       "          \"excerpt\": \"Transportation and airfield infrastructure data sourced from PAVER, BUILDER, RAILER, FHWA Bridge Repor"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ValueError",
        "evalue": "LLM return invalid JSON\nExcerpt around error (pos 0)\n```json\n{\n  \"schema_version\": \"2.5\",\n  \"run_id\": \"2026-01-28T20:43:04Z__bb5893\",\n  \"question\": \"What utilitites condition data exists across STAMP products?\",\n  \"retrieval_summary\": {\n    \"families_considered\": [\n      \"E\",\n      \"I\",\n      \"M\"\n    ]\n\nRaw output (first 1200 chars)\n```json\n{\n  \"schema_version\": \"2.5\",\n  \"run_id\": \"2026-01-28T20:43:04Z__bb5893\",\n  \"question\": \"What utilitites condition data exists across STAMP products?\",\n  \"retrieval_summary\": {\n    \"families_considered\": [\n      \"E\",\n      \"I\",\n      \"M\"\n    ],\n    \"chunks_used\": 12,\n    \"coverage_by_family\": {\n      \"E\": 0,\n      \"I\": 6,\n      \"M\": 6\n    }\n  },\n  \"claims\": [\n    {\n      \"claim_id\": \"C1\",\n      \"claim_text\": \"I-STAMP does not contain specific utilities condition data; it focuses on transportation and airfield infrastructure sourced from PAVER, BUILDER, RAILER, FHWA Bridge Report, and RPA Export through August 2025.\",\n      \"claim_type\": \"factual\",\n      \"stamp_family\": \"I\",\n      \"stamp_products\": [\n        \"All\"\n      ],\n      \"confidence\": \"high\",\n      \"support\": [\n        {\n          \"citation_id\": \"S1\",\n          \"excerpt\": \"Transportation and airfield infrastructure data sourced from PAVER, BUILDER, RAILER, FHWA Bridge Report, and RPA Export through August 2025.\",\n          \"relevance\": \"direct\"\n        },\n        {\n          \"citation_id\": \"S2\",\n          \"excerpt\": \"Transportation and airfield infrastructure data sourced from PAVER, BUILDER, RAILER, FHWA Bridge Repor"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: LLM return invalid JSON\nExcerpt around error (pos 0)\n```json\n{\n  \"schema_version\": \"2.5\",\n  \"run_id\": \"2026-01-28T20:43:04Z__bb5893\",\n  \"question\": \"What utilitites condition data exists across STAMP products?\",\n  \"retrieval_summary\": {\n    \"families_considered\": [\n      \"E\",\n      \"I\",\n      \"M\"\n    ]\n\nRaw output (first 1200 chars)\n```json\n{\n  \"schema_version\": \"2.5\",\n  \"run_id\": \"2026-01-28T20:43:04Z__bb5893\",\n  \"question\": \"What utilitites condition data exists across STAMP products?\",\n  \"retrieval_summary\": {\n    \"families_considered\": [\n      \"E\",\n      \"I\",\n      \"M\"\n    ],\n    \"chunks_used\": 12,\n    \"coverage_by_family\": {\n      \"E\": 0,\n      \"I\": 6,\n      \"M\": 6\n    }\n  },\n  \"claims\": [\n    {\n      \"claim_id\": \"C1\",\n      \"claim_text\": \"I-STAMP does not contain specific utilities condition data; it focuses on transportation and airfield infrastructure sourced from PAVER, BUILDER, RAILER, FHWA Bridge Report, and RPA Export through August 2025.\",\n      \"claim_type\": \"factual\",\n      \"stamp_family\": \"I\",\n      \"stamp_products\": [\n        \"All\"\n      ],\n      \"confidence\": \"high\",\n      \"support\": [\n        {\n          \"citation_id\": \"S1\",\n          \"excerpt\": \"Transportation and airfield infrastructure data sourced from PAVER, BUILDER, RAILER, FHWA Bridge Report, and RPA Export through August 2025.\",\n          \"relevance\": \"direct\"\n        },\n        {\n          \"citation_id\": \"S2\",\n          \"excerpt\": \"Transportation and airfield infrastructure data sourced from PAVER, BUILDER, RAILER, FHWA Bridge Repor\n[Trace ID: 00-76f139f1c68cb342689a4d2b2ce466c9-69fe3de3c3375cc2-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-5522249380927693>, line 17\u001B[0m, in \u001B[0;36msafe_json_loads\u001B[0;34m(s)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 17\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m json\u001B[38;5;241m.\u001B[39mloads(candidate)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m json\u001B[38;5;241m.\u001B[39mJSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m/usr/lib/python3.12/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_decoder\u001B[38;5;241m.\u001B[39mdecode(s)\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
        "File \u001B[0;32m/usr/lib/python3.12/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03mcontaining a JSON document).\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw_decode(s, idx\u001B[38;5;241m=\u001B[39m_w(s, \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mend())\n\u001B[1;32m    338\u001B[0m end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n",
        "File \u001B[0;32m/usr/lib/python3.12/json/decoder.py:355\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m--> 355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj, end\n",
        "\u001B[0;31mJSONDecodeError\u001B[0m: Expecting value: line 1 column 1 (char 0)",
        "\nDuring handling of the above exception, another exception occurred:\n",
        "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
        "File \u001B[0;32m<command-1479720345622387>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# out = ask_stamp_xscope(\"what utilities risks are consistent across the enterprise, installations, and MAJCOMs and where do they differ?\")\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m out \u001B[38;5;241m=\u001B[39m ask_stamp_xscope(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat utilitites condition data exists across STAMP products?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# out = ask_stamp_xscope(\"Across scopes, what are the top recurring drivers of infrastructure risk?\")\u001B[39;00m\n\u001B[1;32m      4\u001B[0m display(out[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmarkdown\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
        "File \u001B[0;32m<command-1479720345622385>, line 171\u001B[0m, in \u001B[0;36mask_stamp_xscope\u001B[0;34m(question, endpoint, top_k_each)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;66;03m#answer = llm_call_fn(prompt, endpoint=endpoint, temperature=0.2, max_tokens=1100)\u001B[39;00m\n\u001B[1;32m    169\u001B[0m raw \u001B[38;5;241m=\u001B[39m llm_call_fn(v25_prompt, endpoint\u001B[38;5;241m=\u001B[39mendpoint, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, max_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1800\u001B[39m)\n\u001B[0;32m--> 171\u001B[0m v25 \u001B[38;5;241m=\u001B[39m safe_json_loads(raw)\n\u001B[1;32m    173\u001B[0m \u001B[38;5;66;03m# return {\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;66;03m#     \"answer\": answer,\u001B[39;00m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;66;03m#     \"citations\": all_citations,\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;66;03m#     \"debug\": {\"version\": version, \"product\": product, \"counts\": {\"E\": len(e_rows), \"I\": len(i_rows), \"M\": len(m_rows)}}\u001B[39;00m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# }\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv25\u001B[39m\u001B[38;5;124m\"\u001B[39m: v25,\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmarkdown\u001B[39m\u001B[38;5;124m\"\u001B[39m: render_v25_markdown(v25),\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdebug\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion\u001B[39m\u001B[38;5;124m\"\u001B[39m: version, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproduct\u001B[39m\u001B[38;5;124m\"\u001B[39m: product, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcounts\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlen\u001B[39m(e_rows), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlen\u001B[39m(i_rows), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mM\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlen\u001B[39m(m_rows)}}\n\u001B[1;32m    183\u001B[0m }\n",
        "File \u001B[0;32m<command-5522249380927693>, line 22\u001B[0m, in \u001B[0;36msafe_json_loads\u001B[0;34m(s)\u001B[0m\n\u001B[1;32m     20\u001B[0m end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(e\u001B[38;5;241m.\u001B[39mpos \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m250\u001B[39m, \u001B[38;5;28mlen\u001B[39m(candidate))\n\u001B[1;32m     21\u001B[0m excerpt \u001B[38;5;241m=\u001B[39m candidate[start:end]\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLLM return invalid JSON\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExcerpt around error (pos \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;241m.\u001B[39mpos\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mexcerpt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaw output (first 1200 chars)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mtxt[:\u001B[38;5;241m1200\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mValueError\u001B[0m: LLM return invalid JSON\nExcerpt around error (pos 0)\n```json\n{\n  \"schema_version\": \"2.5\",\n  \"run_id\": \"2026-01-28T20:43:04Z__bb5893\",\n  \"question\": \"What utilitites condition data exists across STAMP products?\",\n  \"retrieval_summary\": {\n    \"families_considered\": [\n      \"E\",\n      \"I\",\n      \"M\"\n    ]\n\nRaw output (first 1200 chars)\n```json\n{\n  \"schema_version\": \"2.5\",\n  \"run_id\": \"2026-01-28T20:43:04Z__bb5893\",\n  \"question\": \"What utilitites condition data exists across STAMP products?\",\n  \"retrieval_summary\": {\n    \"families_considered\": [\n      \"E\",\n      \"I\",\n      \"M\"\n    ],\n    \"chunks_used\": 12,\n    \"coverage_by_family\": {\n      \"E\": 0,\n      \"I\": 6,\n      \"M\": 6\n    }\n  },\n  \"claims\": [\n    {\n      \"claim_id\": \"C1\",\n      \"claim_text\": \"I-STAMP does not contain specific utilities condition data; it focuses on transportation and airfield infrastructure sourced from PAVER, BUILDER, RAILER, FHWA Bridge Report, and RPA Export through August 2025.\",\n      \"claim_type\": \"factual\",\n      \"stamp_family\": \"I\",\n      \"stamp_products\": [\n        \"All\"\n      ],\n      \"confidence\": \"high\",\n      \"support\": [\n        {\n          \"citation_id\": \"S1\",\n          \"excerpt\": \"Transportation and airfield infrastructure data sourced from PAVER, BUILDER, RAILER, FHWA Bridge Report, and RPA Export through August 2025.\",\n          \"relevance\": \"direct\"\n        },\n        {\n          \"citation_id\": \"S2\",\n          \"excerpt\": \"Transportation and airfield infrastructure data sourced from PAVER, BUILDER, RAILER, FHWA Bridge Repor"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# out = ask_stamp_xscope(\"what utilities risks are consistent across the enterprise, installations, and MAJCOMs and where do they differ?\")\n",
    "out = ask_stamp_xscope(\"What utilitites condition data exists across STAMP products?\")\n",
    "# out = ask_stamp_xscope(\"Across scopes, what are the top recurring drivers of infrastructure risk?\")\n",
    "display(out[\"markdown\"])\n",
    "out[\"v25\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c807057b-b8d6-40e0-ba2a-84a087d4d2b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1479720345622383,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Stamps AI_v2.5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}